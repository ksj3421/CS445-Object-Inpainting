{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdc6bc9-1343-4542-9664-2a100cc3cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class FaceCompletionDataset(Dataset):\n",
    "    def __init__(self, image_dir, block_size=(64, 64)):\n",
    "        self.image_dir = image_dir\n",
    "        self.block_size = block_size\n",
    "        self.image_list = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def random_block_mask(self, image, block_size=(64, 64)):\n",
    "        h, w, _ = image.shape\n",
    "        x = random.randint(0, w - block_size[0])\n",
    "        y = random.randint(0, h - block_size[1])\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        mask[y:y + block_size[1], x:x + block_size[0]] = 1\n",
    "        masked_image = image.copy()\n",
    "        noise = np.random.randint(0, 256, size=(block_size[1], block_size[0], 3), dtype=np.uint8)\n",
    "        masked_image[y:y + block_size[1], x:x + block_size[0]] = noise\n",
    "        return masked_image, mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_list[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        masked_image, mask = self.random_block_mask(image)\n",
    "\n",
    "        image = ToTensor()(image)\n",
    "        mask = ToTensor()(mask)\n",
    "        masked_image = ToTensor()(masked_image)\n",
    "\n",
    "        return image, mask, masked_image\n",
    "\n",
    "    \n",
    "\n",
    "image_dir = \"preprocessed_images/\"\n",
    "\n",
    "celeba_dataset = FaceCompletionDataset(image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45341fc9-4405-4122-bfc6-7bc36a123d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Assuming your_dataset is an instance of your Dataset class\n",
    "train_size = int(0.8 * len(celeba_dataset))  # Use 80% of the dataset for training\n",
    "val_size = len(celeba_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(celeba_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05737bb0-f5a0-4b3f-92a6-4aafa1ba01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8406591d-6769-4b3e-9d5a-5f52542afc78",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "vgg19 = models.vgg19()\n",
    "model_weights_path = 'vgg19-dcbb9e9d.pth'\n",
    "vgg19.load_state_dict(torch.load(model_weights_path))\n",
    "vgg19 = vgg19.to(device)\n",
    "vgg19.eval()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb2de883-cc61-4436-946d-2b36b7b2e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        vgg19 = models.vgg19()\n",
    "        model_weights_path = 'vgg19-dcbb9e9d.pth'\n",
    "        vgg19.load_state_dict(torch.load(model_weights_path))\n",
    "        vgg19 = vgg19.to(device)\n",
    "        vgg19.eval()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *list(vgg19.features.children())[:18],  # conv1 to pool3 of VGG-19\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4 * 8 * 8 * 256, 4096),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4096, 8 * 8 * 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Unflatten(1, (256, 8, 8)),\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=4, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, in_channels, kernel_size=5, stride=1, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.decoder(x1)\n",
    "        return torch.sigmoid(x2)\n",
    "\n",
    "# refer from dcgan's discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 32 * 32, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "d_criterion=nn.BCEWithLogitsLoss()\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = d_criterion(real_output, torch.ones_like(real_output).to(device))\n",
    "    fake_loss = d_criterion(fake_output, torch.zeros_like(fake_output).to(device))\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d01a316-c1a7-4081-81dd-d1379fb72d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm as spectral_norm_fn\n",
    "from torch.nn.utils import weight_norm as weight_norm_fn\n",
    "\n",
    "class Parser(nn.Module):\n",
    "    def __init__(self, config): # config is not used..\n",
    "        super(Parser, self).__init__()\n",
    "        self.input_dim = 3\n",
    "        self.class_num = 17      # (16 components + backgorund)\n",
    "        self.cnum = 64\n",
    "\n",
    "        # 128 * 128 * cnum\n",
    "        self.conv1_1 = gen_conv(self.input_dim, self.cnum, 7, 1, 1,)\n",
    "        self.conv1_2 = gen_conv(self.cnum, self.cnum, 3, 2, 1)\n",
    "        #--self.pool1\n",
    "\n",
    "        # 64 * 64 * cnum\n",
    "        self.conv2_1 = gen_conv(self.cnum, self.cnum * 2, 3, 1, 1)\n",
    "        self.conv2_2 = gen_conv(self.cnum * 2, self.cnum * 2, 3, 2, 1)\n",
    "        #--self.pool2\n",
    "\n",
    "        # 32 * 32 * cnum\n",
    "        self.conv3_1  = gen_conv(self.cnum * 2, self.cnum * 4, 3, 1, 1)\n",
    "        self.conv3_2 = gen_conv(self.cnum * 4, self.cnum * 4, 3, 1, 1)\n",
    "        self.conv3_3 = gen_conv(self.cnum * 4, self.cnum * 4, 3, 2, 1)\n",
    "        #--self.pool3\n",
    "\n",
    "        # 16 * 16 * cnum\n",
    "        self.conv4_1 = gen_conv(self.cnum * 4, self.cnum * 8, 3, 1, 1)\n",
    "        self.conv4_2 = gen_conv(self.cnum * 8, self.cnum * 8, 3, 1, 1)\n",
    "        self.conv4_3 = gen_conv(self.cnum * 8, self.cnum * 8, 3, 2, 1)\n",
    "        #--self.pool4\n",
    "\n",
    "        # 8 * 8 * cnum\n",
    "        self.conv5_1 = gen_conv(self.cnum * 8, self.cnum * 8, 3, 1, 1)\n",
    "        self.conv5_2 = gen_conv(self.cnum * 8, self.cnum * 8, 3, 1, 1)\n",
    "        self.conv5_3 = gen_conv(self.cnum * 8, self.cnum * 8, 3, 2, 1)\n",
    "        #--self.pool5\n",
    "\n",
    "        # 4 * 4 * cnum\n",
    "        self.conv6_1 = gen_conv(self.cnum * 8, 4096, 3, 1, 1)\n",
    "\n",
    "\n",
    "        #-- self.deconv6\n",
    "        self.conv7_1  = gen_conv(4096, self.cnum * 8, 5, 1, 2)\n",
    "        #-- dropout6\n",
    "\n",
    "        # 8 * 8 * cnum\n",
    "        #-- self.deconv7\n",
    "        self.conv8_1 = gen_conv(self.cnum * 8, self.cnum * 8, 5, 1, 2)\n",
    "        # -- dropout7\n",
    "\n",
    "        # 16 * 16 * cnum\n",
    "        #--self.deconv8\n",
    "        self.conv9_1 = gen_conv(self.cnum * 8, self.cnum * 8, 5, 1, 2)\n",
    "        # -- dropout8\n",
    "\n",
    "        # 32 * 32 * cnum\n",
    "        #-- self.deconv9\n",
    "        self.conv10_1 = gen_conv(self.cnum * 8, self.cnum * 4, 5, 1, 2)\n",
    "        # -- dropout9\n",
    "\n",
    "        # 64 * 64 * cnum\n",
    "        #--self.deconv10\n",
    "        self.conv11_1 = gen_conv(self.cnum * 4, self.cnum * 2, 3, 1, 1)\n",
    "        # -- dropout10\n",
    "\n",
    "        # 128 * 128 * cnum\n",
    "        #--self.deconv11\n",
    "        self.conv12_1 = gen_conv(self.cnum * 2, self.cnum, 3, 1, 1)\n",
    "        # -- dropout11\n",
    "\n",
    "        # 128 * 128 * C+1 (FINAL)\n",
    "        self.h_out = gen_conv(self.cnum, self.class_num, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # conv1\n",
    "        x = self.conv1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        #print(x.shape)\n",
    "        #x = nn.MaxPool2d(x)\n",
    "\n",
    "\n",
    "        # conv2\n",
    "        x = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        #print(x.shape)\n",
    "        #x = nn.MaxPool2d(x)\n",
    "\n",
    "\n",
    "        # conv3\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.conv3_3(x)\n",
    "        #print(x.shape)\n",
    "        #x = nn.MaxPool2d(x)\n",
    "\n",
    "        # conv4\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.conv4_2(x)\n",
    "        x = self.conv4_3(x)\n",
    "        #print(x.shape)\n",
    "        #x = nn.MaxPool2d(x)\n",
    "\n",
    "        # conv5\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.conv5_2(x)\n",
    "        x = self.conv5_3(x)\n",
    "        #print(x.shape)\n",
    "        #x = nn.MaxPool2d(x)\n",
    "\n",
    "        # conv6\n",
    "        x = self.conv6_1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # conv7\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.conv7_1(x)\n",
    "        #print(x.shape)\n",
    "        x = F.dropout2d(x)\n",
    "\n",
    "        # conv8\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.conv8_1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # conv9\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.conv9_1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # conv10\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.conv10_1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # conv11\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.conv11_1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # conv12\n",
    "        #x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.conv12_1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # h_out\n",
    "        x_out = self.h_out(x)\n",
    "        #print(x_out.shape)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "\n",
    "def gen_conv(input_dim, output_dim, kernel_size=3, stride=1, padding=0, rate=1,\n",
    "             activation='elu'):\n",
    "    return Conv2dBlock(input_dim, output_dim, kernel_size, stride,\n",
    "                       conv_padding=padding, dilation=rate,\n",
    "                       activation=activation)\n",
    "\n",
    "\n",
    "def dis_conv(input_dim, output_dim, kernel_size=5, stride=2, padding=0, rate=1,\n",
    "             activation='lrelu'):\n",
    "    return Conv2dBlock(input_dim, output_dim, kernel_size, stride,\n",
    "                       conv_padding=padding, dilation=rate,\n",
    "                       activation=activation)\n",
    "\n",
    "\n",
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel_size, stride, padding=0,\n",
    "                 conv_padding=0, dilation=1, name='conv', weight_norm='sn', norm='none',\n",
    "                 activation='relu', pad_type='zero', transpose=False):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        self.use_bias = True\n",
    "        # initialize padding\n",
    "        if pad_type == 'reflect':\n",
    "            self.pad = nn.ReflectionPad2d(padding)\n",
    "        elif pad_type == 'replicate':\n",
    "            self.pad = nn.ReplicationPad2d(padding)\n",
    "        elif pad_type == 'zero':\n",
    "            self.pad = nn.ZeroPad2d(padding)\n",
    "        elif pad_type == 'none':\n",
    "            self.pad = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
    "\n",
    "        # initialize normalization\n",
    "        norm_dim = output_dim\n",
    "        if norm == 'bn':\n",
    "            self.norm = nn.BatchNorm2d(norm_dim)\n",
    "        elif norm == 'in':\n",
    "            self.norm = nn.InstanceNorm2d(norm_dim)\n",
    "        elif norm == 'none':\n",
    "            self.norm = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
    "\n",
    "        if weight_norm == 'sn':\n",
    "            self.weight_norm = spectral_norm_fn\n",
    "        elif weight_norm == 'wn':\n",
    "            self.weight_norm = weight_norm_fn\n",
    "        elif weight_norm == 'none':\n",
    "            self.weight_norm = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported normalization: {}\".format(weight_norm)\n",
    "\n",
    "        # initialize activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU(inplace=True)\n",
    "        elif activation == 'lrelu':\n",
    "            self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = nn.PReLU()\n",
    "        elif activation == 'selu':\n",
    "            self.activation = nn.SELU(inplace=True)\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'none':\n",
    "            self.activation = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
    "\n",
    "        # initialize convolution\n",
    "        if transpose:\n",
    "            self.conv = nn.ConvTranspose2d(input_dim, output_dim,\n",
    "                                           kernel_size, stride,\n",
    "                                           padding=conv_padding,\n",
    "                                           output_padding=conv_padding,\n",
    "                                           dilation=dilation,\n",
    "                                           bias=self.use_bias)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride,\n",
    "                                  padding=conv_padding, dilation=dilation,\n",
    "                                  bias=self.use_bias)\n",
    "\n",
    "        if self.weight_norm:\n",
    "            self.conv = self.weight_norm(self.conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pad:\n",
    "            x = self.conv(self.pad(x))\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a342bdbf-8843-45f3-98f1-b29fe5117772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config CONFIG] [--seed SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/x1112373/.local/share/jupyter/runtime/kernel-0c80c9f8-f0c2-4aad-8f25-a460abce06cb.json\n"
     ]
    }
   ],
   "source": [
    "#from utils.network_seg_contour import Parser\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import yaml\n",
    "def get_config(config_file):\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "def get_args():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--config', type=str, default='config/seg_config.yaml', help=\"training configuration\")\n",
    "    parser.add_argument('--seed', type=int, default=2023, help='manual seed')\n",
    "\n",
    "    try:\n",
    "        args = parser.parse_args()\n",
    "    except SystemExit:\n",
    "        args = Namespace(config='config/seg_config.yaml', seed=2023)\n",
    "\n",
    "    return args\n",
    "\n",
    "args = get_args()\n",
    "config = get_config(args.config)\n",
    "netG = Parser(config)\n",
    "def load_face_parsing_model(model_path):\n",
    "    \n",
    "    netG.load_state_dict(torch.load(model_path), strict=False)\n",
    "    netG.eval()\n",
    "    return netG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f8c3a1e-8e6f-4a1c-80c6-bdfa734a8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "generator = Generator()\n",
    "\n",
    "global_discriminator = Discriminator()\n",
    "local_discriminator = Discriminator()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_adv = nn.BCEWithLogitsLoss()\n",
    "criterion_context = nn.L1Loss()\n",
    "\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=0.0001) ## generate converge faster than discriminator\n",
    "discriminator_optimizer =torch.optim.Adam(global_discriminator.parameters(), lr=0.0001)\n",
    "criterion_perceptual = nn.L1Loss()\n",
    "\n",
    "face_parsing_model = load_face_parsing_model('pretrained_model/parser_00100000.pt')\n",
    "face_parsing_model = face_parsing_model.to(device)\n",
    "criterion_parsing = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d928cdc8-c87f-47b2-9cc5-6ae199b4d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(device)\n",
    "discriminator_global = global_discriminator.to(device)\n",
    "discriminator_local = local_discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe58ae82-fc14-4445-87ff-b311c129b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "def evaluate_models(generator, discriminator_global, discriminator_local, dataloader, criterion_adv, criterion_context, criterion_perceptual, vgg19, lambda1, lambda2, lambda3):\n",
    "    generator.eval()\n",
    "    discriminator_global.eval()\n",
    "    discriminator_local.eval()\n",
    "\n",
    "    total_gen_loss = 0\n",
    "    total_disc_global_loss = 0\n",
    "    total_disc_local_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks, masked_images) in enumerate(dataloader):\n",
    "            if i == 0:\n",
    "                print(i)\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            masked_images = masked_images.to(device) \n",
    "\n",
    "            completed_images = generator(masked_images)\n",
    "            parsed_images = face_parsing_model(images)\n",
    "            parsed_completed_images = face_parsing_model(completed_images)\n",
    "            loss_parsing = criterion_parsing(parsed_completed_images, torch.argmax(parsed_images, dim=1))\n",
    "            \n",
    "            expanded_masks = masks.expand_as(images)\n",
    "            masked_completed_images = completed_images * expanded_masks\n",
    "        \n",
    "            local_real_output = local_discriminator(masked_images)\n",
    "            local_fake_output = local_discriminator(masked_completed_images)\n",
    "        \n",
    "            global_real_output = global_discriminator(images)\n",
    "            global_fake_output = global_discriminator(completed_images)\n",
    "\n",
    "\n",
    "            loss_adv_global = criterion_adv(global_real_output, real_labels_expanded) + criterion_adv(global_fake_output, fake_labels_expanded)\n",
    "            loss_adv_local = criterion_adv(local_real_output, real_labels_expanded) + criterion_adv(local_fake_output, fake_labels_expanded)\n",
    "            loss_adv = loss_adv_global + loss_adv_local\n",
    "\n",
    "            loss_context = criterion_context(completed_images * expanded_masks, images * expanded_masks)\n",
    " \n",
    "            loss_perceptual = criterion_perceptual(vgg19(completed_images), vgg19(images))\n",
    "            parsed_images = face_parsing_model(images)\n",
    "            parsed_completed_images = face_parsing_model(completed_images)        \n",
    "            loss_parsing = criterion_parsing(parsed_completed_images, torch.argmax(parsed_images, dim=1))\n",
    "\n",
    "            \n",
    "            loss_generator = lambda1 * loss_adv + lambda2 * loss_context + loss_perceptual + lambda3 * loss_parsing\n",
    "            \n",
    "            total_gen_loss += loss_generator.item()\n",
    "            total_disc_global_loss += loss_adv_global.item()\n",
    "            total_disc_local_loss += loss_adv_local.item()\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    mean_gen_loss = total_gen_loss / total_samples\n",
    "    mean_disc_global_loss = total_disc_global_loss / total_samples\n",
    "    mean_disc_local_loss = total_disc_local_loss / total_samples\n",
    "\n",
    "    return mean_gen_loss, mean_disc_global_loss, mean_disc_local_loss\n",
    "\n",
    "def evaluate_model_external(model, dataloader):\n",
    "    model.eval()\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks, masked_images) in enumerate(dataloader):\n",
    "            masked_images = masked_images.to(device) \n",
    "            completed_images = model(masked_images)\n",
    " \n",
    "            for i in range(images.size(0)):\n",
    "                image = images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                completed_image = completed_images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "                total_psnr += psnr(image, completed_image)\n",
    "                total_ssim += ssim(image, completed_image, multichannel=True, win_size=3, data_range=1)\n",
    "\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    return total_psnr / total_samples, total_ssim / total_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f6308a4-b70a-4c30-85b1-1acfaae17a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "def return_image_numpy(images, scale=False):\n",
    "    if scale:\n",
    "        completed_images_np = images[0].cpu().detach().numpy()\n",
    " \n",
    "        completed_images_np = ((completed_images_np + 1) * 127.5)\n",
    "        print(completed_images_np.shape)\n",
    "        return completed_images_np.transpose(1, 2, 0)\n",
    "    else:\n",
    "        return images[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "save_path = './model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1015c25-1223-4ee1-8c2d-caa51bd93d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "num_epochs = 30\n",
    "best_loss = float('inf')\n",
    "patience = 3\n",
    "# Set the weights for the different losses\n",
    "lambda1 = 300\n",
    "lambda2 = 300\n",
    "lambda3 = 0.005\n",
    "\n",
    "result_df = pd.DataFrame(columns= ['epoch', 'step', 'val_gen_loss', 'val_disc_global_loss', 'val_disc_local_loss', 'val_psnr', 'val_ssim' ])\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, masks, masked_images) in enumerate(train_dataloader):\n",
    "        # Resize the images and masks to a consistent size\n",
    "        \n",
    "        images = images.to(device)\n",
    "        masked_images = masked_images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        # Train the generator and discriminators using the images and masks\n",
    "        # Implement the training algorithm according to the paper\n",
    "    \n",
    "        # 1. Generate completed images using the generator (Completion Network)\n",
    "\n",
    "        completed_images = generator(masked_images)\n",
    " \n",
    "        # 2. Compute the adversarial loss for the generator using the global and local discriminators\n",
    "#         real_labels = torch.ones(images.size(0), 1).to(device)\n",
    "#         fake_labels = torch.zeros(images.size(0), 1).to(device)\n",
    " \n",
    "        # Expand the real and fake labels to match the output size of the discriminators\n",
    "#         real_labels_expanded = real_labels.view(real_labels.size(0), 1, 1, 1).expand_as(global_real_output).clone()\n",
    "#         fake_labels_expanded = fake_labels.view(fake_labels.size(0), 1, 1, 1).expand_as(global_fake_output).clone()\n",
    "        \n",
    "#         real_labels_expanded = real_labels_expanded.to(device)\n",
    "#         fake_labels_expanded = fake_labels_expanded.to(device)\n",
    "        \n",
    "#         loss_adv_global = criterion_adv(global_real_output, real_labels_expanded) + criterion_adv(global_fake_output, fake_labels_expanded)\n",
    "#         loss_adv_local = criterion_adv(local_real_output, real_labels_expanded) + criterion_adv(local_fake_output, fake_labels_expanded)\n",
    "#         loss_adv = loss_adv_global + loss_adv_local\n",
    "        \n",
    "        # 3. Compute the contextual loss by measuring the L1 distance between the original and completed images\n",
    "        expanded_masks = masks.expand_as(images)\n",
    "        masked_completed_images = completed_images * expanded_masks\n",
    "        \n",
    "        local_real_output = local_discriminator(masked_images)\n",
    "        local_fake_output = local_discriminator(masked_completed_images)\n",
    "        \n",
    "        global_real_output = global_discriminator(images)\n",
    "        global_fake_output = global_discriminator(completed_images)\n",
    "        \n",
    "        loss_adv_global = discriminator_loss(global_real_output, global_fake_output)\n",
    "        loss_adv_local = discriminator_loss(local_real_output, local_fake_output)\n",
    "        loss_adv = loss_adv_global + loss_adv_local\n",
    "\n",
    "        \n",
    "        loss_context = criterion_context(completed_images * expanded_masks, images * expanded_masks)\n",
    "\n",
    "        # 4. Compute the perceptual loss using the VGG-16 network\n",
    "        # Resize completed_images and images to be compatible with VGG-16\n",
    "#         vgg_completed_images = upsample_for_vgg(completed_images)\n",
    "#         vgg_images = upsample_for_vgg(images)\n",
    "\n",
    "        loss_perceptual = criterion_perceptual(vgg19(completed_images), vgg19(images))\n",
    "        parsed_images = face_parsing_model(images)\n",
    "        parsed_completed_images = face_parsing_model(completed_images)\n",
    "        loss_parsing = criterion_parsing(parsed_completed_images, torch.argmax(parsed_images, dim=1))\n",
    "\n",
    "        # 5. Update the weights of the generator using the combined loss (adversarial, contextual, and perceptual losses)\n",
    "        loss_generator = lambda1 * loss_adv + lambda2 * loss_context + loss_perceptual + lambda3 * loss_parsing\n",
    "\n",
    "        optimizer_generator.zero_grad()\n",
    "        loss_generator.backward(retain_graph=True)\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # 6. Update the weights of the global and local discriminators using their respective adversarial losses\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        loss_adv.backward()\n",
    "\n",
    "        discriminator_optimizer.step()\n",
    "        if i % 10000 == 0:\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "            axs[0].imshow(return_image_numpy(images))\n",
    "            axs[0].set_title(\"Original Image\")\n",
    "            axs[1].imshow(return_image_numpy(masked_images))\n",
    "            axs[1].set_title(\"masked_images\")\n",
    "            axs[2].imshow(return_image_numpy(masks))\n",
    "            axs[2].set_title(\"masks\")\n",
    "            axs[3].imshow(return_image_numpy(completed_images))\n",
    "            axs[3].set_title(\"completed_images image\")\n",
    "\n",
    "            for ax in axs:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluation\n",
    "    # Inside your training loop, after each epoch\n",
    "    val_gen_loss, val_disc_global_loss, val_disc_local_loss = evaluate_models(generator, discriminator_global, discriminator_local, validation_dataloader, criterion_adv, criterion_context, criterion_perceptual, vgg19, lambda1, lambda2, lambda3)\n",
    "    print(f\"Epoch: {epoch}, Validation Losses - Generator: {val_gen_loss:.4f}, Discriminator Global: {val_disc_global_loss:.4f}, Discriminator Local: {val_disc_local_loss:.4f}\")\n",
    "    val_psnr, val_ssim = evaluate_model_external(generator, validation_dataloader)\n",
    "    print(f\"Epoch: {epoch}, Validation PSNR: {val_psnr:.4f}, SSIM: {val_ssim:.4f}\")\n",
    "    result_df.append({'epoch': epoch, 'step': i, 'val_gen_loss': val_gen_loss, 'val_disc_global_loss': val_disc_global_loss, 'val_disc_local_loss': val_disc_local_loss, 'val_psnr': val_psnr, 'val_ssim': val_ssim}, ignore_index=True)\n",
    "    result_df.to_csv('result_df.csv') \n",
    "    # Update the best loss and save the model if necessary\n",
    "    \n",
    "    if val_gen_loss < best_loss:\n",
    "        best_loss = val_gen_loss\n",
    "        best_generator = copy.deepcopy(generator.state_dict())\n",
    "        best_discriminator_global = copy.deepcopy(discriminator_global.state_dict())\n",
    "        best_discriminator_local = copy.deepcopy(discriminator_local.state_dict())\n",
    "        # Save the best models\n",
    "        torch.save(best_generator, os.path.join(save_path, 'generator.pth'))\n",
    "        torch.save(best_discriminator_global, os.path.join(save_path, 'discriminator_global.pth'))\n",
    "        torch.save(best_discriminator_local, os.path.join(save_path, 'discriminator_local.pth'))\n",
    "\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    # Check for early stopping\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68a942-5d6e-4c45-9467-365c1d43f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('result_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9dab79-2520-4a94-9e8a-252d0486757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Display the original image, mask, and masked image\n",
    "# fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "# axs[0].imshow(return_image_numpy(images))\n",
    "# axs[0].set_title(\"Original Image\")\n",
    "# axs[1].imshow(return_image_numpy(masked_images))\n",
    "# axs[1].set_title(\"masked_images\")\n",
    "# axs[2].imshow(return_image_numpy(masks))\n",
    "# axs[2].set_title(\"masks\")\n",
    "# axs[3].imshow(return_image_numpy(completed_images))\n",
    "# axs[3].set_title(\"completed_images image\")\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])\n",
    "\n",
    "\n",
    "#images, masks, masked_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba58d6-1b04-4097-9ed2-b598d50fce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfcc32-67a6-47d1-97eb-2c579cbcf9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
