{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdc6bc9-1343-4542-9664-2a100cc3cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202599\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "class CelebADatasetWithPreGeneratedMasks(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = os.listdir(img_dir)\n",
    "        self.masked_image_files = os.listdir(mask_dir)\n",
    "        print(len(self.masked_image_files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.image_files[idx])  # Assumes mask files have same names as image files\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"1\")  # Loads the mask as binary (black and white)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Usage:\n",
    "img_dir = \"data_faces/img_align_celeba/\"\n",
    "mask_dir = \"data_faces/masked_images/\"\n",
    "\n",
    "celeba_dataset = CelebADatasetWithPreGeneratedMasks(img_dir, mask_dir, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45341fc9-4405-4122-bfc6-7bc36a123d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Assuming your_dataset is an instance of your Dataset class\n",
    "train_size = int(0.8 * len(celeba_dataset))  # Use 80% of the dataset for training\n",
    "val_size = len(celeba_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(celeba_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05737bb0-f5a0-4b3f-92a6-4aafa1ba01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2de883-cc61-4436-946d-2b36b7b2e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "\n",
    "# refer from https://github.com/jaxony/unet-pytorch/blob/master/model.py\n",
    "def conv3x3(in_channels, out_channels, stride=1, \n",
    "            padding=1, bias=True, groups=1):    \n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        bias=bias,\n",
    "        groups=groups)\n",
    "\n",
    "def upconv2x2(in_channels, out_channels, mode='transpose'):\n",
    "    if mode == 'transpose':\n",
    "        return nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "    else:\n",
    "        # out_channels is always going to be the same\n",
    "        # as in_channels\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "            conv1x1(in_channels, out_channels))\n",
    "\n",
    "def conv1x1(in_channels, out_channels, groups=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=1,\n",
    "        groups=groups,\n",
    "        stride=1)\n",
    "\n",
    "def crop_tensors(t1, t2):\n",
    "    _, _, h1, w1 = t1.size()\n",
    "    _, _, h2, w2 = t2.size()\n",
    "    h_crop, w_crop = min(h1, h2), min(w1, w2)\n",
    "\n",
    "    t1_cropped = t1[:, :, :h_crop, :w_crop]\n",
    "    t2_cropped = t2[:, :, :h_crop, :w_crop]\n",
    "\n",
    "    return t1_cropped, t2_cropped\n",
    "\n",
    "class DownConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 convolutions and 1 MaxPool.\n",
    "    A ReLU activation follows each convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, pooling=True):\n",
    "        super(DownConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.conv1 = conv3x3(self.in_channels, self.out_channels)\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "        if self.pooling:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        before_pool = x\n",
    "        if self.pooling:\n",
    "            x = self.pool(x)\n",
    "        return x, before_pool\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, merge_mode='concat', up_mode='transpose'):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.merge_mode = merge_mode\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        if up_mode == 'transpose':\n",
    "            self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.upconv = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        if merge_mode == 'concat':\n",
    "            self.conv = nn.Conv2d(in_channels + out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        else:  # merge_mode == 'add'\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pad = nn.ReflectionPad2d(1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, from_down, from_up):\n",
    "        from_down = F.interpolate(from_down, size=from_up.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        if self.merge_mode == 'concat':\n",
    "            x = torch.cat((from_up, from_down), 1)\n",
    "        else:\n",
    "            x = from_up + from_down\n",
    "        x = F.relu(self.bn(self.conv(x)))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, depth=5, start_filts=64, up_mode='transpose', merge_mode='concat'):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.down_convs = []\n",
    "        self.up_convs = []\n",
    "\n",
    "        # create the encoder pathway and add to a list\n",
    "        for i in range(depth):\n",
    "            ins = in_channels if i == 0 else outs\n",
    "            outs = start_filts * (2 ** i)\n",
    "            pooling = True if i < depth - 1 else False\n",
    "\n",
    "            down_conv = DownConv(ins, outs, pooling=pooling)\n",
    "            self.down_convs.append(down_conv)\n",
    "\n",
    "        # create the decoder pathway and add to a list\n",
    "        # - careful! decoding only requires depth-1 blocks\n",
    "        for i in range(depth - 1):\n",
    "            ins = outs\n",
    "            outs = ins // 2\n",
    "            up_conv = UpConv(ins, outs, up_mode=up_mode, merge_mode=merge_mode)\n",
    "            self.up_convs.append(up_conv)\n",
    "\n",
    "        self.conv_final = conv1x1(outs, num_classes)\n",
    "\n",
    "        # add the list of modules to current module\n",
    "        self.down_convs = nn.ModuleList(self.down_convs)\n",
    "        self.up_convs = nn.ModuleList(self.up_convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_outs = []\n",
    "\n",
    "        # encoder pathway, save outputs for merging\n",
    "        for i, module in enumerate(self.down_convs):\n",
    "            x, before_pool = module(x)\n",
    "            encoder_outs.append(before_pool)\n",
    "\n",
    "        for i, module in enumerate(self.up_convs):\n",
    "            before_pool = encoder_outs[-(i + 2)]\n",
    "            x = module(before_pool, x)\n",
    "\n",
    "        x = self.conv_final(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "## refer from https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/pix2pix/models.py\n",
    "# However, in the context of image inpainting, we usually have one input, which is the generated/completed image, or the real image for comparison. \n",
    "# so i modify the discriminator to accept a single input and process it accordingly. \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True, stride=2):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=stride, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=False))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512, stride=1),  # Change stride to 1\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8c3a1e-8e6f-4a1c-80c6-bdfa734a8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "generator = Generator(num_classes=3, in_channels=3, depth=5, start_filts=64, up_mode='transpose', merge_mode='concat')\n",
    "discriminator_global = Discriminator(in_channels=3)\n",
    "discriminator_local = Discriminator(in_channels=3)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_adv = nn.BCEWithLogitsLoss()\n",
    "criterion_context = nn.L1Loss()\n",
    "\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_discriminator_global = torch.optim.Adam(discriminator_global.parameters(), lr=0.0002)\n",
    "optimizer_discriminator_local = torch.optim.Adam(discriminator_local.parameters(), lr=0.0002)\n",
    "criterion_perceptual = nn.L1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c622aa6e-8d47-4a0b-9b25-1c9cf58d1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(device)\n",
    "discriminator_global = discriminator_global.to(device)\n",
    "discriminator_local = discriminator_local.to(device)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9da4fe-6301-4a5a-abdb-4087675cc756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "vgg16 = models.vgg16(pretrained=False).features\n",
    "vgg16 = vgg16.to(device)\n",
    "vgg16.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe58ae82-fc14-4445-87ff-b311c129b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "def evaluate_models(generator, discriminator_global, discriminator_local, validation_dataloader, criterion_adv, criterion_context, criterion_perceptual, vgg16):\n",
    "    generator.eval()\n",
    "    discriminator_global.eval()\n",
    "    discriminator_local.eval()\n",
    "\n",
    "    total_gen_loss = 0\n",
    "    total_disc_global_loss = 0\n",
    "    total_disc_local_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in validation_dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            completed_images = generator(images * masks)\n",
    "\n",
    "            real_labels = torch.ones(images.size(0), 1).to(device)\n",
    "            fake_labels = torch.zeros(images.size(0), 1).to(device)\n",
    "\n",
    "            global_real_output = discriminator_global(upsample(images))\n",
    "            global_fake_output = discriminator_global(upsampled_completed_images)\n",
    "\n",
    "            local_real_output = discriminator_local(images * upsampled_masks)\n",
    "            local_fake_output = discriminator_local(upsampled_completed_images * upsampled_masks)\n",
    "\n",
    "            # Expand the real and fake labels to match the output size of the discriminators\n",
    "            real_labels_expanded = real_labels.view(real_labels.size(0), 1, 1, 1).expand_as(global_real_output).clone()\n",
    "            fake_labels_expanded = fake_labels.view(fake_labels.size(0), 1, 1, 1).expand_as(global_fake_output).clone()\n",
    "\n",
    "            real_labels_expanded = real_labels_expanded.to(device)\n",
    "            fake_labels_expanded = fake_labels_expanded.to(device)\n",
    "            \n",
    "            loss_adv_global = criterion_adv(global_real_output, real_labels_expanded) + criterion_adv(global_fake_output, fake_labels_expanded)\n",
    "            loss_adv_local = criterion_adv(local_real_output, real_labels_expanded) + criterion_adv(local_fake_output, fake_labels_expanded)\n",
    "            loss_adv = loss_adv_global + loss_adv_local\n",
    "\n",
    "            expanded_masks = masks.expand_as(images)\n",
    "            loss_context = criterion_context(upsampled_completed_images * expanded_masks, images * expanded_masks)\n",
    "\n",
    "            completed_images = F.interpolate(completed_images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "            loss_perceptual = criterion_perceptual(vgg16(completed_images), vgg16(images))\n",
    "\n",
    "            loss_generator = loss_adv + loss_context + loss_perceptual\n",
    "\n",
    "            total_gen_loss += loss_generator.item()\n",
    "            total_disc_global_loss += loss_adv_global.item()\n",
    "            total_disc_local_loss += loss_adv_local.item()\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    mean_gen_loss = total_gen_loss / total_samples\n",
    "    mean_disc_global_loss = total_disc_global_loss / total_samples\n",
    "    mean_disc_local_loss = total_disc_local_loss / total_samples\n",
    "\n",
    "    return mean_gen_loss, mean_disc_global_loss, mean_disc_local_loss\n",
    "\n",
    "def evaluate_model_external(model, dataloader):\n",
    "    model.eval()\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "\n",
    "            images_resized = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            masks_resized = F.interpolate(masks, size=(224, 224), mode='nearest')\n",
    "\n",
    "            images_resized = images_resized.to(device)\n",
    "            masks_resized = masks_resized.to(device)\n",
    "            completed_images_resized = model(images_resized * masks_resized)\n",
    "\n",
    "            # Resize the completed images back to the original size\n",
    "            completed_images = F.interpolate(completed_images_resized, size=(218, 178), mode='bilinear', align_corners=False)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                image = images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                completed_image = completed_images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "                total_psnr += psnr(image, completed_image)\n",
    "                total_ssim += ssim(image, completed_image, multichannel=True, win_size=3, data_range=1)\n",
    "\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    return total_psnr / total_samples, total_ssim / total_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1015c25-1223-4ee1-8c2d-caa51bd93d07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40928/2721335648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_adv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_context\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_perceptual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mloss_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0moptimizer_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "best_loss = float('inf')\n",
    "\n",
    "upsample = nn.Upsample(size=(218, 178), mode='bilinear', align_corners=True)\n",
    "upsample_masks = nn.Upsample(size=(218, 178), mode='nearest')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, masks) in enumerate(train_dataloader):\n",
    "        # Resize the images and masks to a consistent size\n",
    "        \n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Train the generator and discriminators using the images and masks\n",
    "        # Implement the training algorithm according to the paper\n",
    "    \n",
    "        # 1. Generate completed images using the generator (Completion Network)\n",
    "\n",
    "        completed_images = generator(images * masks)\n",
    " \n",
    "        # 2. Compute the adversarial loss for the generator using the global and local discriminators\n",
    "        real_labels = torch.ones(images.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(images.size(0), 1).to(device)\n",
    "\n",
    "        upsampled_completed_images = upsample(completed_images)\n",
    "        upsampled_masks = upsample_masks(1 - masks)\n",
    "\n",
    "        local_real_output = discriminator_local(images * upsampled_masks)\n",
    "        local_fake_output = discriminator_local(upsampled_completed_images * upsampled_masks)\n",
    "\n",
    "        global_real_output = discriminator_global(upsample(images))\n",
    "        global_fake_output = discriminator_global(upsampled_completed_images)\n",
    "\n",
    "        \n",
    "        # Expand the real and fake labels to match the output size of the discriminators\n",
    "        real_labels_expanded = real_labels.view(real_labels.size(0), 1, 1, 1).expand_as(global_real_output).clone()\n",
    "        fake_labels_expanded = fake_labels.view(fake_labels.size(0), 1, 1, 1).expand_as(global_fake_output).clone()\n",
    "        \n",
    "        real_labels_expanded = real_labels_expanded.to(device)\n",
    "        fake_labels_expanded = fake_labels_expanded.to(device)\n",
    "        \n",
    "        loss_adv_global = criterion_adv(global_real_output, real_labels_expanded) + criterion_adv(global_fake_output, fake_labels_expanded)\n",
    "        loss_adv_local = criterion_adv(local_real_output, real_labels_expanded) + criterion_adv(local_fake_output, fake_labels_expanded)\n",
    "        loss_adv = loss_adv_global + loss_adv_local\n",
    "        \n",
    "        # 3. Compute the contextual loss by measuring the L1 distance between the original and completed images\n",
    "        expanded_masks = masks.expand_as(images)\n",
    "        loss_context = criterion_context(upsampled_completed_images * expanded_masks, images * expanded_masks)\n",
    "\n",
    "        # 4. Compute the perceptual loss using the VGG-16 network\n",
    "        # Resize completed_images and images to be compatible with VGG-16\n",
    "        completed_images = F.interpolate(completed_images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        loss_perceptual = criterion_perceptual(vgg16(completed_images), vgg16(images))\n",
    "\n",
    "        # 5. Update the weights of the generator using the combined loss (adversarial, contextual, and perceptual losses)\n",
    "        loss_generator = loss_adv + loss_context + loss_perceptual\n",
    "        optimizer_generator.zero_grad()\n",
    "        loss_generator.backward(retain_graph=True)\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # 6. Update the weights of the global and local discriminators using their respective adversarial losses\n",
    "        optimizer_discriminator_global.zero_grad()\n",
    "        loss_adv_global.backward(retain_graph=True)\n",
    "\n",
    "        optimizer_discriminator_local.zero_grad()\n",
    "        loss_adv_local.backward()\n",
    "\n",
    "        optimizer_discriminator_global.step()\n",
    "        optimizer_discriminator_local.step()\n",
    "    # Evaluation\n",
    "    # Inside your training loop, after each epoch\n",
    "    val_gen_loss, val_disc_global_loss, val_disc_local_loss = evaluate_models(generator, discriminator_global, discriminator_local, validation_dataloader, criterion_adv, criterion_context, criterion_perceptual, vgg16)\n",
    "    print(f\"Epoch: {epoch}, Validation Losses - Generator: {val_gen_loss:.4f}, Discriminator Global: {val_disc_global_loss:.4f}, Discriminator Local: {val_disc_local_loss:.4f}\")\n",
    "    val_psnr, val_ssim = evaluate_model_external(generator, validation_dataloader)\n",
    "    print(f\"Epoch: {epoch}, Validation PSNR: {val_psnr:.4f}, SSIM: {val_ssim:.4f}\")\n",
    "    # Update the best loss and save the model if necessary\n",
    "    \n",
    "    if val_gen_loss < best_loss:\n",
    "        best_loss = val_gen_loss\n",
    "        best_generator = deepcopy(generator.state_dict())\n",
    "        best_discriminator_global = deepcopy(discriminator_global.state_dict())\n",
    "        best_discriminator_local = deepcopy(discriminator_local.state_dict())\n",
    "        # Save the best models\n",
    "        torch.save(best_generator, os.path.join(save_path, 'generator.pth'))\n",
    "        torch.save(best_discriminator_global, os.path.join(save_path, 'discriminator_global.pth'))\n",
    "        torch.save(best_discriminator_local, os.path.join(save_path, 'discriminator_local.pth'))\n",
    "\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    # Check for early stopping\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd0e83-c7b6-43d8-a43f-7e3224f1d4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
