{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdc6bc9-1343-4542-9664-2a100cc3cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class FaceCompletionDataset(Dataset):\n",
    "    def __init__(self, image_dir, block_size=(64, 64)):\n",
    "        self.image_dir = image_dir\n",
    "        self.block_size = block_size\n",
    "        self.image_list = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def random_block_mask(self, image, block_size=(64, 64)):\n",
    "        h, w, _ = image.shape\n",
    "        x = random.randint(0, w - block_size[0])\n",
    "        y = random.randint(0, h - block_size[1])\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        mask[y:y + block_size[1], x:x + block_size[0]] = 1\n",
    "        masked_image = image.copy()\n",
    "        noise = np.random.randint(0, 256, size=(block_size[1], block_size[0], 3), dtype=np.uint8)\n",
    "        masked_image[y:y + block_size[1], x:x + block_size[0]] = noise\n",
    "        return masked_image, mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_list[idx])\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        masked_image, mask = self.random_block_mask(image)\n",
    "\n",
    "        image = ToTensor()(image)\n",
    "        mask = ToTensor()(mask)\n",
    "        masked_image = ToTensor()(masked_image)\n",
    "\n",
    "        return image, mask, masked_image\n",
    "\n",
    "    \n",
    "\n",
    "image_dir = \"preprocessed_images/\"\n",
    "\n",
    "celeba_dataset = FaceCompletionDataset(image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45341fc9-4405-4122-bfc6-7bc36a123d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Assuming your_dataset is an instance of your Dataset class\n",
    "train_size = int(0.8 * len(celeba_dataset))  # Use 80% of the dataset for training\n",
    "val_size = len(celeba_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(celeba_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05737bb0-f5a0-4b3f-92a6-4aafa1ba01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8406591d-6769-4b3e-9d5a-5f52542afc78",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "vgg19 = models.vgg19()\n",
    "model_weights_path = 'vgg19-dcbb9e9d.pth'\n",
    "vgg19.load_state_dict(torch.load(model_weights_path))\n",
    "vgg19 = vgg19.to(device)\n",
    "vgg19.eval()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb2de883-cc61-4436-946d-2b36b7b2e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        vgg19 = models.vgg19()\n",
    "        model_weights_path = 'vgg19-dcbb9e9d.pth'\n",
    "        vgg19.load_state_dict(torch.load(model_weights_path))\n",
    "        vgg19 = vgg19.to(device)\n",
    "        vgg19.eval()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *list(vgg19.features.children())[:18],  # conv1 to pool3 of VGG-19\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4 * 8 * 8 * 256, 4096),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4096, 8 * 8 * 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Unflatten(1, (256, 8, 8)),\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=4, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, in_channels, kernel_size=5, stride=1, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.decoder(x1)\n",
    "        return torch.sigmoid(x2)\n",
    "\n",
    "## refer from https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/pix2pix/models.py\n",
    "# However, in the context of image inpainting, we usually have one input, which is the generated/completed image, or the real image for comparison. \n",
    "# so i modify the discriminator to accept a single input and process it accordingly. \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True, stride=2):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=stride, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=False))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512, stride=1),  # Change stride to 1\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8c3a1e-8e6f-4a1c-80c6-bdfa734a8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "generator = Generator()\n",
    "\n",
    "discriminator_global = Discriminator(in_channels=3)\n",
    "discriminator_local = Discriminator(in_channels=3)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_adv = nn.BCEWithLogitsLoss()\n",
    "criterion_context = nn.L1Loss()\n",
    "\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_discriminator_global = torch.optim.Adam(discriminator_global.parameters(), lr=0.0002)\n",
    "optimizer_discriminator_local = torch.optim.Adam(discriminator_local.parameters(), lr=0.0002)\n",
    "criterion_perceptual = nn.L1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d928cdc8-c87f-47b2-9cc5-6ae199b4d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(device)\n",
    "discriminator_global = discriminator_global.to(device)\n",
    "discriminator_local = discriminator_local.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe58ae82-fc14-4445-87ff-b311c129b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "def evaluate_models(generator, discriminator_global, discriminator_local, dataloader, criterion_adv, criterion_context, criterion_perceptual, vgg19):\n",
    "    generator.eval()\n",
    "    discriminator_global.eval()\n",
    "    discriminator_local.eval()\n",
    "\n",
    "    total_gen_loss = 0\n",
    "    total_disc_global_loss = 0\n",
    "    total_disc_local_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks, masked_images) in enumerate(dataloader):\n",
    "            if i == 0:\n",
    "                print(i)\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            masked_images = masked_images.to(device) \n",
    "\n",
    "            completed_images = generator(masked_images)\n",
    "\n",
    "            real_labels = torch.ones(images.size(0), 1).to(device)\n",
    "            fake_labels = torch.zeros(images.size(0), 1).to(device)\n",
    "\n",
    "            local_real_output = discriminator_local(images)\n",
    "            local_fake_output = discriminator_local(completed_images)\n",
    "\n",
    "            global_real_output = discriminator_global(images)\n",
    "            global_fake_output = discriminator_global(completed_images)\n",
    "\n",
    "            # Expand the real and fake labels to match the output size of the discriminators\n",
    "            real_labels_expanded = real_labels.view(real_labels.size(0), 1, 1, 1).expand_as(global_real_output).clone()\n",
    "            fake_labels_expanded = fake_labels.view(fake_labels.size(0), 1, 1, 1).expand_as(global_fake_output).clone()\n",
    "\n",
    "            real_labels_expanded = real_labels_expanded.to(device)\n",
    "            fake_labels_expanded = fake_labels_expanded.to(device)\n",
    "\n",
    "            loss_adv_global = criterion_adv(global_real_output, real_labels_expanded) + criterion_adv(global_fake_output, fake_labels_expanded)\n",
    "            loss_adv_local = criterion_adv(local_real_output, real_labels_expanded) + criterion_adv(local_fake_output, fake_labels_expanded)\n",
    "            loss_adv = loss_adv_global + loss_adv_local\n",
    "\n",
    "            expanded_masks = masks.expand_as(images)\n",
    "            loss_context = criterion_context(completed_images * expanded_masks, images * expanded_masks)\n",
    " \n",
    "            loss_perceptual = criterion_perceptual(vgg19(completed_images), vgg19(images))\n",
    "\n",
    "            loss_generator = loss_adv + loss_context + loss_perceptual\n",
    "\n",
    "            total_gen_loss += loss_generator.item()\n",
    "            total_disc_global_loss += loss_adv_global.item()\n",
    "            total_disc_local_loss += loss_adv_local.item()\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    mean_gen_loss = total_gen_loss / total_samples\n",
    "    mean_disc_global_loss = total_disc_global_loss / total_samples\n",
    "    mean_disc_local_loss = total_disc_local_loss / total_samples\n",
    "\n",
    "    return mean_gen_loss, mean_disc_global_loss, mean_disc_local_loss\n",
    "\n",
    "def evaluate_model_external(model, dataloader):\n",
    "    model.eval()\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks, masked_images) in enumerate(dataloader):\n",
    "            masked_images = masked_images.to(device) \n",
    "            completed_images = model(masked_images)\n",
    " \n",
    "            for i in range(images.size(0)):\n",
    "                image = images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                completed_image = completed_images[i].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "                total_psnr += psnr(image, completed_image)\n",
    "                total_ssim += ssim(image, completed_image, multichannel=True, win_size=3, data_range=1)\n",
    "\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    return total_psnr / total_samples, total_ssim / total_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f6308a4-b70a-4c30-85b1-1acfaae17a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "def return_image_numpy(images, scale=False):\n",
    "    if scale:\n",
    "        completed_images_np = images[0].cpu().detach().numpy()\n",
    " \n",
    "        completed_images_np = ((completed_images_np + 1) * 127.5)\n",
    "        print(completed_images_np.shape)\n",
    "        return completed_images_np.transpose(1, 2, 0)\n",
    "    else:\n",
    "        return images[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "save_path = './model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1015c25-1223-4ee1-8c2d-caa51bd93d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "num_epochs = 30\n",
    "best_loss = float('inf')\n",
    "patience = 3\n",
    "result_df = pd.DataFrame(columns= ['epoch', 'step', 'val_gen_loss', 'val_disc_global_loss', 'val_disc_local_loss', 'val_psnr', 'val_ssim' ])\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, masks, masked_images) in enumerate(train_dataloader):\n",
    "        # Resize the images and masks to a consistent size\n",
    "        \n",
    "        images = images.to(device)\n",
    "        masked_images = masked_images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        # Train the generator and discriminators using the images and masks\n",
    "        # Implement the training algorithm according to the paper\n",
    "    \n",
    "        # 1. Generate completed images using the generator (Completion Network)\n",
    "\n",
    "        completed_images = generator(masked_images)\n",
    " \n",
    "        # 2. Compute the adversarial loss for the generator using the global and local discriminators\n",
    "        real_labels = torch.ones(images.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(images.size(0), 1).to(device)\n",
    " \n",
    "        local_real_output = discriminator_local(images)\n",
    "        local_fake_output = discriminator_local(completed_images)\n",
    "\n",
    "        global_real_output = discriminator_global(images)\n",
    "        global_fake_output = discriminator_global(completed_images)\n",
    "\n",
    "        \n",
    "        # Expand the real and fake labels to match the output size of the discriminators\n",
    "        real_labels_expanded = real_labels.view(real_labels.size(0), 1, 1, 1).expand_as(global_real_output).clone()\n",
    "        fake_labels_expanded = fake_labels.view(fake_labels.size(0), 1, 1, 1).expand_as(global_fake_output).clone()\n",
    "        \n",
    "        real_labels_expanded = real_labels_expanded.to(device)\n",
    "        fake_labels_expanded = fake_labels_expanded.to(device)\n",
    "        \n",
    "        loss_adv_global = criterion_adv(global_real_output, real_labels_expanded) + criterion_adv(global_fake_output, fake_labels_expanded)\n",
    "        loss_adv_local = criterion_adv(local_real_output, real_labels_expanded) + criterion_adv(local_fake_output, fake_labels_expanded)\n",
    "        loss_adv = loss_adv_global + loss_adv_local\n",
    "        \n",
    "        # 3. Compute the contextual loss by measuring the L1 distance between the original and completed images\n",
    "        expanded_masks = masks.expand_as(images)\n",
    "        loss_context = criterion_context(completed_images * expanded_masks, images * expanded_masks)\n",
    "\n",
    "        # 4. Compute the perceptual loss using the VGG-16 network\n",
    "        # Resize completed_images and images to be compatible with VGG-16\n",
    "#         vgg_completed_images = upsample_for_vgg(completed_images)\n",
    "#         vgg_images = upsample_for_vgg(images)\n",
    "\n",
    "        loss_perceptual = criterion_perceptual(vgg19(completed_images), vgg19(images))\n",
    "\n",
    "        # 5. Update the weights of the generator using the combined loss (adversarial, contextual, and perceptual losses)\n",
    "        loss_generator = loss_adv + loss_context + loss_perceptual\n",
    "        optimizer_generator.zero_grad()\n",
    "        loss_generator.backward(retain_graph=True)\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # 6. Update the weights of the global and local discriminators using their respective adversarial losses\n",
    "        optimizer_discriminator_global.zero_grad()\n",
    "        loss_adv_global.backward(retain_graph=True)\n",
    "\n",
    "        optimizer_discriminator_local.zero_grad()\n",
    "        loss_adv_local.backward()\n",
    "\n",
    "        optimizer_discriminator_global.step()\n",
    "        optimizer_discriminator_local.step()\n",
    "        if i % 10000 == 0:\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "            axs[0].imshow(return_image_numpy(images))\n",
    "            axs[0].set_title(\"Original Image\")\n",
    "            axs[1].imshow(return_image_numpy(masked_images))\n",
    "            axs[1].set_title(\"masked_images\")\n",
    "            axs[2].imshow(return_image_numpy(masks))\n",
    "            axs[2].set_title(\"masks\")\n",
    "            axs[3].imshow(return_image_numpy(completed_images))\n",
    "            axs[3].set_title(\"completed_images image\")\n",
    "\n",
    "            for ax in axs:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluation\n",
    "    # Inside your training loop, after each epoch\n",
    "    val_gen_loss, val_disc_global_loss, val_disc_local_loss = evaluate_models(generator, discriminator_global, discriminator_local, validation_dataloader, criterion_adv, criterion_context, criterion_perceptual, vgg19)\n",
    "    print(f\"Epoch: {epoch}, Validation Losses - Generator: {val_gen_loss:.4f}, Discriminator Global: {val_disc_global_loss:.4f}, Discriminator Local: {val_disc_local_loss:.4f}\")\n",
    "    val_psnr, val_ssim = evaluate_model_external(generator, validation_dataloader)\n",
    "    print(f\"Epoch: {epoch}, Validation PSNR: {val_psnr:.4f}, SSIM: {val_ssim:.4f}\")\n",
    "    result_df.append({'epoch': epoch, 'step': i, 'val_gen_loss': val_gen_loss, 'val_disc_global_loss': val_disc_global_loss, 'val_disc_local_loss': val_disc_local_loss, 'val_psnr': val_psnr, 'val_ssim': val_ssim}, ignore_index=True)\n",
    "    result_df.to_csv('result_df.csv') \n",
    "    # Update the best loss and save the model if necessary\n",
    "    \n",
    "    if val_gen_loss < best_loss:\n",
    "        best_loss = val_gen_loss\n",
    "        best_generator = copy.deepcopy(generator.state_dict())\n",
    "        best_discriminator_global = copy.deepcopy(discriminator_global.state_dict())\n",
    "        best_discriminator_local = copy.deepcopy(discriminator_local.state_dict())\n",
    "        # Save the best models\n",
    "        torch.save(best_generator, os.path.join(save_path, 'generator.pth'))\n",
    "        torch.save(best_discriminator_global, os.path.join(save_path, 'discriminator_global.pth'))\n",
    "        torch.save(best_discriminator_local, os.path.join(save_path, 'discriminator_local.pth'))\n",
    "\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    # Check for early stopping\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68a942-5d6e-4c45-9467-365c1d43f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('result_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9dab79-2520-4a94-9e8a-252d0486757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Display the original image, mask, and masked image\n",
    "# fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "# axs[0].imshow(return_image_numpy(images))\n",
    "# axs[0].set_title(\"Original Image\")\n",
    "# axs[1].imshow(return_image_numpy(masked_images))\n",
    "# axs[1].set_title(\"masked_images\")\n",
    "# axs[2].imshow(return_image_numpy(masks))\n",
    "# axs[2].set_title(\"masks\")\n",
    "# axs[3].imshow(return_image_numpy(completed_images))\n",
    "# axs[3].set_title(\"completed_images image\")\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])\n",
    "\n",
    "\n",
    "#images, masks, masked_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba58d6-1b04-4097-9ed2-b598d50fce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfcc32-67a6-47d1-97eb-2c579cbcf9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
