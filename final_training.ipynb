{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfdc6bc9-1343-4542-9664-2a100cc3cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils.preprocess import FaceCompletionDataset\n",
    "from utils.network_seq_contour import Parser\n",
    "from utils.models import Generator, Discriminator\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45341fc9-4405-4122-bfc6-7bc36a123d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"preprocessed_images/\"\n",
    "save_path = './model_trained/'\n",
    "celeba_dataset = FaceCompletionDataset(image_dir)\n",
    "\n",
    "# train-test split\n",
    "train_size = int(0.8 * len(celeba_dataset))  # Use 80% of the dataset for training\n",
    "val_size = len(celeba_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(celeba_dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(val_dataset, 'val_dataset.pkl') ## preserve val set\n",
    "joblib.dump(train_dataset, 'train_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05737bb0-f5a0-4b3f-92a6-4aafa1ba01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8406591d-6769-4b3e-9d5a-5f52542afc78",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config CONFIG] [--seed SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/x1112373/.local/share/jupyter/runtime/kernel-7b2b1a6e-bc9c-4c33-b6be-90a56fcac0b1.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parser(\n",
       "  (conv1_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv1_2): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (conv2_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv2_2): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (conv3_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv3_2): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv3_3): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (conv4_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv4_2): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv4_3): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (conv5_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv5_2): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv5_3): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (conv6_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(512, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv7_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(4096, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (conv8_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (conv9_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (conv10_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (conv11_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv12_1): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (h_out): Conv2dBlock(\n",
       "    (pad): ZeroPad2d((0, 0, 0, 0))\n",
       "    (activation): ELU(alpha=1.0, inplace=True)\n",
       "    (conv): Conv2d(64, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "## for visualize result\n",
    "def return_image_numpy(images, scale=False):\n",
    "    if scale:\n",
    "        completed_images_np = images[0].cpu().detach().numpy()\n",
    " \n",
    "        completed_images_np = ((completed_images_np + 1) * 127.5)\n",
    "        print(completed_images_np.shape)\n",
    "        return completed_images_np.transpose(1, 2, 0)\n",
    "    else:\n",
    "        return images[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "def adjust_learning_rate(optimizer, gamma=0.1, num_steps=1):\n",
    "    for i in range(num_steps):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= gamma\n",
    "    \n",
    "## for loading parsing network for perceptual loss\n",
    "def get_config(config_file):\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def get_args():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--config', type=str, default='config/seg_config.yaml', help=\"training configuration\")\n",
    "    parser.add_argument('--seed', type=int, default=2023, help='manual seed')\n",
    "\n",
    "    try:\n",
    "        args = parser.parse_args()\n",
    "    except SystemExit:\n",
    "        args = Namespace(config='config/seg_config.yaml', seed=2023)\n",
    "    return args\n",
    "\n",
    "def load_face_parsing_model(model_path):    \n",
    "    netG.load_state_dict(torch.load(model_path), strict=False)\n",
    "    netG.eval()\n",
    "    return netG\n",
    "\n",
    "## load parser network\n",
    "args = get_args()\n",
    "config = get_config(args.config)\n",
    "netG = Parser(config)\n",
    "\n",
    "# load parsing model\n",
    "face_parsing_model = load_face_parsing_model('pretrained_model/parser_00100000.pt')\n",
    "face_parsing_model = face_parsing_model.to(device)\n",
    "face_parsing_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ddbca73-75b6-4cb5-9a9c-37ba4ca02b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, target_real_label=1.0, target_fake_label=0.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(input)\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real).to(input.device)\n",
    "        return self.loss(input, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f8c3a1e-8e6f-4a1c-80c6-bdfa734a8467",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils.models import Generator, Discriminator\n",
    "from utils.networks import GatedGenerator\n",
    "\n",
    "## initialize first model\n",
    "generator = GatedGenerator()\n",
    "discriminator_global = Discriminator(in_channels=3)\n",
    "discriminator_local = Discriminator(in_channels=3)\n",
    "\n",
    "## loss\n",
    "criterion_context = nn.MSELoss(reduction='sum')\n",
    "criterion_adv = GANLoss(target_real_label=0.9, target_fake_label=0.1)\n",
    "criterion_parsing = nn.SmoothL1Loss()\n",
    "criterion_rec = nn.SmoothL1Loss()\n",
    "criterion_ssim = SSIM(window_size = 11)\n",
    "\n",
    "# Create Optimizer\n",
    "optimizer_generator = torch.optim.Adam(generator.parameters(), lr=0.0001)\n",
    "optimizer_discriminator_global = torch.optim.Adam(generator.parameters(), lr=0.0001)\n",
    "optimizer_discriminator_local = torch.optim.Adam(generator.parameters(), lr=0.0001)\n",
    "\n",
    "# Create the schedulers - when there is scheduler, learning rate goes too small value\n",
    "# scheduler_generator = ReduceLROnPlateau(optimizer_generator, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "# scheduler_discriminator_global = ReduceLROnPlateau(optimizer_discriminator_global, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "# scheduler_discriminator_local = ReduceLROnPlateau(optimizer_discriminator_local, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# device setting\n",
    "generator = generator.to(device)\n",
    "discriminator_global = discriminator_global.to(device)\n",
    "discriminator_local = discriminator_local.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9133cf9-833b-4667-b6c2-8402d80e3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(generator, discriminator_local, discriminator_global, path, device):\n",
    "    state = torch.load(path,map_location=device)\n",
    "    generator.load_state_dict(state['G'])\n",
    "    discriminator_global.load_state_dict(state['D_G'])\n",
    "    discriminator_local.load_state_dict(state['D_L'])\n",
    "    print('Loaded checkpoint successfully')\n",
    "    return generator, discriminator_local, discriminator_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca5df6-faad-4e8c-8782-d7f65a22e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from utils.evaluate import evaluate_models, evaluate_model_external\n",
    "resume = True\n",
    "num_epochs = 20\n",
    "best_loss = float('inf')\n",
    "patience = 5\n",
    "lambda_gp = 10  # Gradient penalty weight\n",
    "result_df = pd.DataFrame(columns= ['epoch', 'step', 'val_gen_loss', 'val_disc_global_loss', 'val_disc_local_loss', 'val_psnr', 'val_ssim' ])\n",
    "generator.train()\n",
    "discriminator_local.train()\n",
    "discriminator_global.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, masks, masked_images) in enumerate(train_dataloader):\n",
    "        # Resize the images and masks to a consistent size\n",
    "\n",
    "        images = images.to(device)\n",
    "        masked_images = masked_images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        ## train discriminator\n",
    "        optimizer_discriminator_global.zero_grad()\n",
    "        optimizer_discriminator_local.zero_grad()\n",
    "        optimizer_generator.zero_grad()\n",
    "\n",
    "        first_out, second_out  = generator(images, masks)\n",
    "        first_out_wholeimg = images * (1 - masks) + first_out * masks     \n",
    "        second_out_wholeimg = images * (1 - masks) + second_out * masks \n",
    "\n",
    "        # Compute the adversarial loss for the generator using the global and local discriminators\n",
    "\n",
    "        local_real_D = discriminator_local(images)\n",
    "        local_fake_D = discriminator_local(second_out_wholeimg.detach())\n",
    "\n",
    "        global_real_D = discriminator_global(images)\n",
    "        global_fake_D = discriminator_global(second_out_wholeimg.detach())\n",
    "\n",
    "        loss_local_fake_D = criterion_adv(local_fake_D, target_is_real=False)\n",
    "        loss_local_real_D = criterion_adv(local_real_D, target_is_real=True)\n",
    "\n",
    "        loss_global_fake_D = criterion_adv(global_fake_D, target_is_real=False)\n",
    "        loss_global_real_D = criterion_adv(global_real_D, target_is_real=True)\n",
    "\n",
    "        #gp = gradient_penalty(discriminator_global, images, completed_images)\n",
    "        loss_d = (loss_local_real_D + loss_local_fake_D + loss_global_fake_D + loss_global_real_D) * 0.25 #+ lambda_gp * gp\n",
    "        loss_d.backward(retain_graph=True)        \n",
    "\n",
    "        optimizer_discriminator_global.step()\n",
    "        optimizer_discriminator_local.step()\n",
    "\n",
    "        local_real_output = None\n",
    "        global_real_output = None\n",
    "\n",
    "        # train generator\n",
    "        optimizer_discriminator_global.zero_grad()\n",
    "        optimizer_discriminator_local.zero_grad()\n",
    "        optimizer_generator.zero_grad()\n",
    "\n",
    "        # generator loss \n",
    "        local_fake_output = discriminator_local(second_out_wholeimg)\n",
    "        global_fake_output = discriminator_global(second_out_wholeimg)\n",
    "        fake_D = (local_fake_output + global_fake_output) * 0.5\n",
    "        loss_G = criterion_adv(fake_D, target_is_real=True)\n",
    "\n",
    "        local_fake_output = None\n",
    "        global_fake_output = None\n",
    "        fake_D = None\n",
    "\n",
    "\n",
    "        # generator reconstruction loss\n",
    "        # Reconstruction loss\n",
    "        loss_l1_1 = criterion_rec(first_out_wholeimg, images)\n",
    "        loss_l1_2 = criterion_rec(second_out_wholeimg, images)\n",
    "        loss_ssim_1 = criterion_ssim(first_out_wholeimg, images)\n",
    "        loss_ssim_2 = criterion_ssim(second_out_wholeimg, images)\n",
    "        loss_rec_1 = 0.5 * loss_l1_1 + 0.5 * (1 - loss_ssim_1)\n",
    "        loss_rec_2 = 0.5 * loss_l1_2 + 0.5 * (1 - loss_ssim_2)\n",
    "\n",
    "        lambda_G = 1.0\n",
    "        lambda_rec_1 = 100.0\n",
    "        lambda_rec_2 = 100.0\n",
    "        lambda_per = 10.0\n",
    "\n",
    "        loss_P = criterion_parsing(face_parsing_model(second_out_wholeimg), face_parsing_model(images))\n",
    "        loss_generator = lambda_G * loss_G + lambda_rec_1 * loss_rec_1 + lambda_rec_2 * loss_rec_2 + lambda_per * loss_P\n",
    "        loss_generator.backward(retain_graph=True)\n",
    "        optimizer_generator.step()\n",
    "        if epoch == 5:\n",
    "            adjust_learning_rate(optimizer_discriminator_global)\n",
    "            adjust_learning_rate(optimizer_discriminator_local)\n",
    "            adjust_learning_rate(optimizer_generator)\n",
    "        if epoch == 10:\n",
    "            adjust_learning_rate(optimizer_discriminator_global)\n",
    "            adjust_learning_rate(optimizer_discriminator_local)\n",
    "            adjust_learning_rate(optimizer_generator)\n",
    "        if epoch == 15:\n",
    "            adjust_learning_rate(optimizer_discriminator_global)\n",
    "            adjust_learning_rate(optimizer_discriminator_local)\n",
    "            adjust_learning_rate(optimizer_generator)\n",
    "\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "            axs[0].imshow(return_image_numpy(images))\n",
    "            axs[0].set_title(\"Original Image\")\n",
    "            axs[1].imshow(return_image_numpy(masks))\n",
    "            axs[1].set_title(\"masks\")\n",
    "            axs[2].imshow(return_image_numpy(first_out_wholeimg))\n",
    "            axs[2].set_title(\"first_out_wholeimg\")\n",
    "            axs[3].imshow(return_image_numpy(second_out_wholeimg))\n",
    "            axs[3].set_title(\"second_out_wholeimg\")\n",
    "\n",
    "            for ax in axs:\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "\n",
    "            plt.show()\n",
    "    \n",
    "    # Evaluation\n",
    "    # Inside your training loop, after each epoch\n",
    "    print(f\"Epoch: {epoch}, Step: {i}, Training Losses - Generator: {loss_generator.item()}\") #, Discriminator Global: {loss_adv_global.item()}, Discriminator Local: {loss_adv_local.item()}\")\n",
    "    val_gen_loss, val_disc_global_loss, val_disc_local_loss = evaluate_models(generator, discriminator_global, discriminator_local, validation_dataloader, criterion_rec, criterion_ssim, face_parsing_model, device)\n",
    "    print(f\"Epoch: {epoch}, Validation Losses - Generator: {val_gen_loss:.8f}, Discriminator Global: {val_disc_global_loss:.8f}, Discriminator Local: {val_disc_local_loss:.8f}\")\n",
    "    val_psnr, val_ssim = evaluate_model_external(generator, validation_dataloader, device)\n",
    "    print(f\"Epoch: {epoch}, Validation PSNR: {val_psnr:.8f}, SSIM: {val_ssim:.8f}\")\n",
    "    if epoch == 0:\n",
    "        result_df = pd.DataFrame([{'epoch': epoch, 'step': i, 'val_gen_loss': val_gen_loss, 'val_disc_global_loss': val_disc_global_loss, 'val_disc_local_loss': val_disc_local_loss, 'val_psnr': val_psnr, 'val_ssim': val_ssim}])\n",
    "    else:\n",
    "        aa = pd.DataFrame([{'epoch': epoch, 'step': i, 'val_gen_loss': val_gen_loss, 'val_disc_global_loss': val_disc_global_loss, 'val_disc_local_loss': val_disc_local_loss, 'val_psnr': val_psnr, 'val_ssim': val_ssim}])\n",
    "        result_df = pd.concat([result_df, aa], axis=0)\n",
    "        result_df.to_csv('result.csv') \n",
    "    # Update the best loss and save the model if necessary\n",
    "    \n",
    "    if (val_gen_loss + val_disc_global_loss) < best_loss:\n",
    "        best_loss = (val_gen_loss + val_disc_global_loss)\n",
    "        best_generator = copy.deepcopy(generator.state_dict())\n",
    "        best_discriminator_global = copy.deepcopy(discriminator_global.state_dict())\n",
    "        best_discriminator_local = copy.deepcopy(discriminator_local.state_dict())\n",
    "        torch.save({\n",
    "                'D_G': best_discriminator_global,\n",
    "                'D_L': best_discriminator_local,\n",
    "                'G': best_generator,\n",
    "        }, os.path.join(\"model_trained\", f\"model_{epoch}.pth\"))\n",
    "        \n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    # Check for early stopping\n",
    "    if counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
